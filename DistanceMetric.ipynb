{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Mean Discrepancy\n",
    "\n",
    "$$\n",
    "MMD^{2} (X,Y) = \\frac{1}{m(m-1)}\\sum_i\\sum_{j\\neq i} k(x_i , x_j) - 2  \\frac{1}{m \\times m}\\sum_i\\sum_{j} k(x_i , y_j) +  \\frac{1}{m(m-1)}\\sum_i\\sum_{j\\neq i} k(y_i , y_j)\n",
    "$$\n",
    "\n",
    "### Using Gaussian Kernel\n",
    "\n",
    "$$\n",
    "k(x_i, x_j) = exp( \\frac{-\\parallel x_i - x_j \\parallel ^2}{2 \\sigma ^2}) = exp(\\frac{1}{\\sigma ^2}[x_i^T x_i - 2x_i^T x_j + x_j^T x_j])\n",
    "$$\n",
    "\n",
    "### Using Matrix to improve computation efficiency\n",
    "\n",
    "$$\n",
    "x = \\begin{pmatrix}\n",
    "x_{1}\\\\\n",
    "x_{2}\\\\\n",
    "x_{3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_ix_j = x \\cdot x^T = \\begin{pmatrix}\n",
    "x_{1,1}& x_{1,2} & x_{1,3}\\\\\n",
    "x_{2,1}& x_{2,2} & x_{2,3}\\\\\n",
    "x_{3,1}& x_{3,2} & x_{3,3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Getting the diagonal components and form a 3x3 matrix. This is equivalent to\n",
    "$$\n",
    "x_i^T x_i = rx = \n",
    "\\begin{pmatrix}\n",
    "x_{1,1}& x_{1,1} & x_{1,1}\\\\\n",
    "x_{2,2}& x_{2,2} & x_{2,2}\\\\\n",
    "x_{3,3}& x_{3,3} & x_{3,3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_j^T x_j = rx^T = \n",
    "\\begin{pmatrix}\n",
    "x_{1,1}& x_{2,2} & x_{3,3}\\\\\n",
    "x_{1,1}& x_{2,2} & x_{3,3}\\\\\n",
    "x_{1,1}& x_{2,2} & x_{3,3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Kernel $k(x,x)$ can be calculated:\n",
    "\n",
    "$$\n",
    "k(x,x) = x_i^T x_i + x_j^T x_j - 2 x_i x_j\n",
    "$$\n",
    "```python\n",
    "xx = torch.mm(x, x.T)\n",
    "rx = xx.diag().unsqueeze(0).expand_as(xx)\n",
    "dxx = rx.T + rx - 2 * xx\n",
    "\n",
    "```\n",
    "https://discuss.pytorch.org/t/maximum-mean-discrepancy-mmd-and-radial-basis-function-rbf/1875/2\n",
    "\n",
    "https://towardsdatascience.com/write-markdown-latex-in-the-jupyter-notebook-10985edb91fd\n",
    "\n",
    "https://medium.com/@aditya.rastogi/the-denominator-inside-the-log-in-the-nt-xent-loss-function-explained-b412eeceba2f\n",
    "\n",
    "https://discuss.pytorch.org/t/build-your-own-loss-function-in-pytorch/235/6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NT-Xent loss\n",
    "$$\n",
    "\\mathcal{L}_{i,j} = - log \\frac{exp(sim(z_i,z_j)/\\tau)}{\\sum_{i=1}^{2N} \\mathbb{1}_{[i \\neq k]} exp(sim(z_i,z_k))}\n",
    "$$\n",
    "\n",
    "### To get cosine similarity, we can do outer product of vector x\n",
    "\n",
    "$$\n",
    "x = \\begin{pmatrix}\n",
    "x_{1}\\\\\n",
    "x_{2}\\\\\n",
    "x_{3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x \\cdot x^T = \\begin{pmatrix}\n",
    "x_{1,1}& x_{1,2} & x_{1,3}\\\\\n",
    "x_{2,1}& x_{2,2} & x_{2,3}\\\\\n",
    "x_{3,1}& x_{3,2} & x_{3,3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "And get the diagonal components\n",
    "\n",
    "```python\n",
    "xx = torch.mm(x, x.T)\n",
    "xx = xx.diag().unsqueeze(0)\n",
    "```\n",
    "\n",
    "### For sim($z_i$, $z_k$) where $i \\neq k $, we will need to get the off diagonal components\n",
    "\n",
    "$$\n",
    "xy = \\begin{pmatrix}\n",
    "x_{1}\\\\\n",
    "x_{2}\\\\\n",
    "x_{3}\\\\\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "y_{3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "xy \\cdot xy^T =\\begin{pmatrix}\n",
    "x_{1}x_{1}& x_{1}x_{2} & \\dots & x_{1}y_{6}\\\\\n",
    "\\vdots & \\ddots & \\ddots &  \\vdots \\\\\n",
    "y_{6}x_{1}  & \\cdots &  & y_{6}y_{6}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "### Few ways to compute similarity matrix\n",
    "\n",
    "1. \n",
    "```python\n",
    "sim = nn.CosineSimilarity(dim=2)(xx.unsqueeze(1),xx.unsqueeze(0))\n",
    "```\n",
    "2.\n",
    "```python\n",
    "xy = torch.cat([x,y])\n",
    "xy = F.normalize(xy,dim=1)\n",
    "sim = torch.mm(xx,xx.T)\n",
    "```\n",
    "\n",
    "### Getting the off-diagonal components\n",
    "```python\n",
    "sim_i_j = torch.diag(sim, batch_size)\n",
    "sim_j_i = torch.diag(sim, -batch_size)\n",
    "```\n",
    "\n",
    "Mask can be created to filter $i = k$ \n",
    "\n",
    "```python\n",
    "ones = torch.ones(batch_size)\n",
    "mask = torch.diagflat(ones, 4) + torch.diagflat(ones, -4)\n",
    "i_not_k = sim - (sim * mask)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise Distance\n",
    "\n",
    "Given a matrix of N x d\n",
    "\n",
    "$$\n",
    "x = \\begin{pmatrix}\n",
    "x_{0}\\\\\n",
    "x_{1}\\\\\n",
    "x_{2}\\\\\n",
    "\\end{pmatrix}\n",
    "y = \\begin{pmatrix}\n",
    "y_{0}\\\\\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "where $x_{i}, y_{i}$ is given to be the row feature vector\n",
    "\n",
    "Pairwise distance matrix dist[i,j] is the square norm between x[i,:] and y[j,:]. To vectorize the calculation, we can expand the matrix into 3D, where each component correspond to $i^{th}$ and $j^{th}$ component of matrix x and y\n",
    "$$\n",
    "\\begin{bmatrix} \\begin{pmatrix}\n",
    "x_{0}\\\\\n",
    "x_{0}\\\\\n",
    "x_{0}\\\\\n",
    "\\end{pmatrix} - \\begin{pmatrix}\n",
    "y_{0}\\\\\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{bmatrix}_{0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \\begin{pmatrix}\n",
    "x_{1}\\\\\n",
    "x_{1}\\\\\n",
    "x_{1}\\\\\n",
    "\\end{pmatrix} - \\begin{pmatrix}\n",
    "y_{0}\\\\\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{bmatrix}_{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \\begin{pmatrix}\n",
    "x_{2}\\\\\n",
    "x_{2}\\\\\n",
    "x_{2}\\\\\n",
    "\\end{pmatrix} - \\begin{pmatrix}\n",
    "y_{0}\\\\\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{bmatrix}_{2}\n",
    "$$\n",
    "\n",
    "```python\n",
    "    x = torch.randn(size=(3,3))\n",
    "    y = torch.randn(size=(3,3))\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "```\n",
    "```python\n",
    "    x = x.unsqueeze(0).expand(n, m, d)\n",
    "```\n",
    "$$\n",
    "\\begin{bmatrix} \\begin{pmatrix}\n",
    "x_{0}\\\\\n",
    "x_{1}\\\\\n",
    "x_{2}\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{bmatrix}_{0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \\begin{pmatrix}\n",
    "x_{0}\\\\\n",
    "x_{1}\\\\\n",
    "x_{2}\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{bmatrix}_{1}\n",
    "$$\n",
    "```python\n",
    "    y = y.unsqueeze(1).expand(n, m, d)\n",
    "```\n",
    "$$\n",
    "\\begin{bmatrix} \\begin{pmatrix}\n",
    "x_{0}\\\\\n",
    "x_{0}\\\\\n",
    "x_{0}\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{bmatrix}_{0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \\begin{pmatrix}\n",
    "x_{1}\\\\\n",
    "x_{1}\\\\\n",
    "x_{1}\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{bmatrix}_{1}\n",
    "$$\n",
    "```python\n",
    "pairwise_distance = torch.pow(x - y, 2).sum(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
